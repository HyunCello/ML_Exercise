{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('pytorch_x86': conda)",
   "metadata": {
    "interpreter": {
     "hash": "3a8cc3801ef01ef31c80d9cd5834424e5f2116e58c72b200c4f5cfba4435f546"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, tensor, max\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss1: 0.3567\nLoss2: 2.3026\n"
     ]
    }
   ],
   "source": [
    "# Cross entropy example\n",
    "# One hot\n",
    "# 0: 1 0 0\n",
    "# 1: 0 1 0\n",
    "# 2: 0 0 1\n",
    "Y = np.array([1, 0, 0])\n",
    "Y_pred1 = np.array([0.7, 0.2, 0.1])\n",
    "Y_pred2 = np.array([0.1, 0.3, 0.6])\n",
    "print(f'Loss1: {np.sum(-Y * np.log(Y_pred1)):.4f}')\n",
    "print(f'Loss2: {np.sum(-Y * np.log(Y_pred2)):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PyTorch Loss1: 0.4170 \nPyTorch Loss2: 1.8406\nY_pred1: 0\nY_pred2: 1\n"
     ]
    }
   ],
   "source": [
    "# Softmax + CrossEntropy (logSoftmax + NLLLoss)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# target is of size nBatch\n",
    "# each element in target has to have 0 <= value < nClasses (0-2)\n",
    "# Input is class, not one-hot\n",
    "Y = tensor([0], requires_grad=False)\n",
    "\n",
    "# input is of size nBatch x nClasses = 1 x 4\n",
    "# Y_pred are logits (not softmax)\n",
    "Y_pred1 = tensor([[2.0, 1.0, 0.1]])\n",
    "Y_pred2 = tensor([[0.5, 2.0, 0.3]])\n",
    "\n",
    "l1 = loss(Y_pred1, Y)\n",
    "l2 = loss(Y_pred2, Y)\n",
    "\n",
    "print(f'PyTorch Loss1: {l1.item():.4f} \\nPyTorch Loss2: {l2.item():.4f}')\n",
    "print(f'Y_pred1: {max(Y_pred1.data, 1)[1].item()}')\n",
    "print(f'Y_pred2: {max(Y_pred2.data, 1)[1].item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Batch Loss1:  0.4966 \nBatch Loss2: 1.2389\n"
     ]
    }
   ],
   "source": [
    "Y = tensor([2, 0, 1], requires_grad=False)\n",
    "\n",
    "# input is of size nBatch x nClasses = 2 x 4\n",
    "# Y_pred are logits (not softmax)\n",
    "Y_pred1 = tensor([[0.1, 0.2, 0.9],\n",
    "                  [1.1, 0.1, 0.2],\n",
    "                  [0.2, 2.1, 0.1]])\n",
    "\n",
    "Y_pred2 = tensor([[0.8, 0.2, 0.3],\n",
    "                  [0.2, 0.3, 0.5],\n",
    "                  [0.2, 0.2, 0.5]])\n",
    "\n",
    "l1 = loss(Y_pred1, Y)\n",
    "l2 = loss(Y_pred2, Y)\n",
    "print(f'Batch Loss1:  {l1.item():.4f} \\nBatch Loss2: {l2.data:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}