{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('pytorch_x86': conda)",
   "metadata": {
    "interpreter": {
     "hash": "3a8cc3801ef01ef31c80d9cd5834424e5f2116e58c72b200c4f5cfba4435f546"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from torch import nn, optim, cuda\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "0it [00:00, ?it/s]Training MNIST Model on cpu\n",
      "============================================\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\n",
      " 99%|█████████▊| 9781248/9912422 [00:04<00:00, 3360862.69it/s]Extracting ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[ADownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "\n",
      "  0%|          | 0/28881 [00:01<?, ?it/s]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[AExtracting ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "\n",
      "\n",
      "  0%|          | 0/1648877 [00:01<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 16384/1648877 [00:01<00:22, 72504.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 49152/1648877 [00:01<00:10, 145860.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 98304/1648877 [00:01<00:07, 214253.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 155648/1648877 [00:01<00:06, 235446.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 303104/1648877 [00:01<00:02, 495462.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 475136/1648877 [00:01<00:01, 776592.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 606208/1648877 [00:02<00:01, 754005.80it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▏    | 860160/1648877 [00:02<00:00, 1084733.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▌   | 1073152/1648877 [00:02<00:00, 1286698.04it/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████▍  | 1220608/1648877 [00:02<00:00, 1245620.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 1359872/1648877 [00:02<00:00, 1119859.76it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|████████▉ | 1482752/1648877 [00:02<00:00, 1009873.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|█████████▉| 1646592/1648877 [00:02<00:00, 1133068.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[AExtracting ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/4542 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[AExtracting ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n",
      "9920512it [00:20, 3360862.69it/s]                             \n",
      "\n",
      "1654784it [00:13, 1133068.08it/s]                             \u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "# Training settings\n",
    "batch_size = 64\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(f'Training MNIST Model on {device}\\n{\"=\" * 44}')\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "32768it [00:17, 1915.38it/s]             \n",
      "1654784it [00:16, 103287.94it/s] \n",
      "8192it [00:12, 631.05it/s]              \n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(784, 520)\n",
    "        self.l2 = nn.Linear(520, 320)\n",
    "        self.l3 = nn.Linear(320, 240)\n",
    "        self.l4 = nn.Linear(240, 120)\n",
    "        self.l5 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)  # Flatten the data (n, 1, 28, 28)-> (n, 784)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        x = F.relu(self.l4(x))\n",
    "        return self.l5(x)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += criterion(output, target).item()\n",
    "        # get the index of the max\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'===========================\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
    "          f'({100. * correct / len(test_loader.dataset):.0f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " | Batch Status: 51200/60000 (85%) | Loss: 0.130805\n",
      "Train Epoch: 6 | Batch Status: 51840/60000 (86%) | Loss: 0.103187\n",
      "Train Epoch: 6 | Batch Status: 52480/60000 (87%) | Loss: 0.227153\n",
      "Train Epoch: 6 | Batch Status: 53120/60000 (88%) | Loss: 0.091589\n",
      "Train Epoch: 6 | Batch Status: 53760/60000 (90%) | Loss: 0.156986\n",
      "Train Epoch: 6 | Batch Status: 54400/60000 (91%) | Loss: 0.241512\n",
      "Train Epoch: 6 | Batch Status: 55040/60000 (92%) | Loss: 0.305819\n",
      "Train Epoch: 6 | Batch Status: 55680/60000 (93%) | Loss: 0.090115\n",
      "Train Epoch: 6 | Batch Status: 56320/60000 (94%) | Loss: 0.094693\n",
      "Train Epoch: 6 | Batch Status: 56960/60000 (95%) | Loss: 0.206353\n",
      "Train Epoch: 6 | Batch Status: 57600/60000 (96%) | Loss: 0.055983\n",
      "Train Epoch: 6 | Batch Status: 58240/60000 (97%) | Loss: 0.112256\n",
      "Train Epoch: 6 | Batch Status: 58880/60000 (98%) | Loss: 0.054385\n",
      "Train Epoch: 6 | Batch Status: 59520/60000 (99%) | Loss: 0.317952\n",
      "Training time: 0m 10s\n",
      "===========================\n",
      "Test set: Average loss: 0.0023, Accuracy: 9568/10000 (96%)\n",
      "Testing time: 0m 11s\n",
      "Train Epoch: 7 | Batch Status: 0/60000 (0%) | Loss: 0.124616\n",
      "Train Epoch: 7 | Batch Status: 640/60000 (1%) | Loss: 0.068601\n",
      "Train Epoch: 7 | Batch Status: 1280/60000 (2%) | Loss: 0.119034\n",
      "Train Epoch: 7 | Batch Status: 1920/60000 (3%) | Loss: 0.109072\n",
      "Train Epoch: 7 | Batch Status: 2560/60000 (4%) | Loss: 0.080796\n",
      "Train Epoch: 7 | Batch Status: 3200/60000 (5%) | Loss: 0.069208\n",
      "Train Epoch: 7 | Batch Status: 3840/60000 (6%) | Loss: 0.072149\n",
      "Train Epoch: 7 | Batch Status: 4480/60000 (7%) | Loss: 0.102004\n",
      "Train Epoch: 7 | Batch Status: 5120/60000 (9%) | Loss: 0.115656\n",
      "Train Epoch: 7 | Batch Status: 5760/60000 (10%) | Loss: 0.044991\n",
      "Train Epoch: 7 | Batch Status: 6400/60000 (11%) | Loss: 0.164627\n",
      "Train Epoch: 7 | Batch Status: 7040/60000 (12%) | Loss: 0.101633\n",
      "Train Epoch: 7 | Batch Status: 7680/60000 (13%) | Loss: 0.037384\n",
      "Train Epoch: 7 | Batch Status: 8320/60000 (14%) | Loss: 0.063318\n",
      "Train Epoch: 7 | Batch Status: 8960/60000 (15%) | Loss: 0.069544\n",
      "Train Epoch: 7 | Batch Status: 9600/60000 (16%) | Loss: 0.095503\n",
      "Train Epoch: 7 | Batch Status: 10240/60000 (17%) | Loss: 0.186595\n",
      "Train Epoch: 7 | Batch Status: 10880/60000 (18%) | Loss: 0.040679\n",
      "Train Epoch: 7 | Batch Status: 11520/60000 (19%) | Loss: 0.057611\n",
      "Train Epoch: 7 | Batch Status: 12160/60000 (20%) | Loss: 0.098620\n",
      "Train Epoch: 7 | Batch Status: 12800/60000 (21%) | Loss: 0.104431\n",
      "Train Epoch: 7 | Batch Status: 13440/60000 (22%) | Loss: 0.097584\n",
      "Train Epoch: 7 | Batch Status: 14080/60000 (23%) | Loss: 0.062512\n",
      "Train Epoch: 7 | Batch Status: 14720/60000 (25%) | Loss: 0.041855\n",
      "Train Epoch: 7 | Batch Status: 15360/60000 (26%) | Loss: 0.164931\n",
      "Train Epoch: 7 | Batch Status: 16000/60000 (27%) | Loss: 0.070473\n",
      "Train Epoch: 7 | Batch Status: 16640/60000 (28%) | Loss: 0.094461\n",
      "Train Epoch: 7 | Batch Status: 17280/60000 (29%) | Loss: 0.088220\n",
      "Train Epoch: 7 | Batch Status: 17920/60000 (30%) | Loss: 0.265457\n",
      "Train Epoch: 7 | Batch Status: 18560/60000 (31%) | Loss: 0.099518\n",
      "Train Epoch: 7 | Batch Status: 19200/60000 (32%) | Loss: 0.162622\n",
      "Train Epoch: 7 | Batch Status: 19840/60000 (33%) | Loss: 0.111771\n",
      "Train Epoch: 7 | Batch Status: 20480/60000 (34%) | Loss: 0.170266\n",
      "Train Epoch: 7 | Batch Status: 21120/60000 (35%) | Loss: 0.235861\n",
      "Train Epoch: 7 | Batch Status: 21760/60000 (36%) | Loss: 0.328164\n",
      "Train Epoch: 7 | Batch Status: 22400/60000 (37%) | Loss: 0.071603\n",
      "Train Epoch: 7 | Batch Status: 23040/60000 (38%) | Loss: 0.109417\n",
      "Train Epoch: 7 | Batch Status: 23680/60000 (39%) | Loss: 0.060246\n",
      "Train Epoch: 7 | Batch Status: 24320/60000 (41%) | Loss: 0.165804\n",
      "Train Epoch: 7 | Batch Status: 24960/60000 (42%) | Loss: 0.062584\n",
      "Train Epoch: 7 | Batch Status: 25600/60000 (43%) | Loss: 0.205173\n",
      "Train Epoch: 7 | Batch Status: 26240/60000 (44%) | Loss: 0.137862\n",
      "Train Epoch: 7 | Batch Status: 26880/60000 (45%) | Loss: 0.040624\n",
      "Train Epoch: 7 | Batch Status: 27520/60000 (46%) | Loss: 0.071710\n",
      "Train Epoch: 7 | Batch Status: 28160/60000 (47%) | Loss: 0.195183\n",
      "Train Epoch: 7 | Batch Status: 28800/60000 (48%) | Loss: 0.209194\n",
      "Train Epoch: 7 | Batch Status: 29440/60000 (49%) | Loss: 0.053782\n",
      "Train Epoch: 7 | Batch Status: 30080/60000 (50%) | Loss: 0.048131\n",
      "Train Epoch: 7 | Batch Status: 30720/60000 (51%) | Loss: 0.092747\n",
      "Train Epoch: 7 | Batch Status: 31360/60000 (52%) | Loss: 0.082869\n",
      "Train Epoch: 7 | Batch Status: 32000/60000 (53%) | Loss: 0.083477\n",
      "Train Epoch: 7 | Batch Status: 32640/60000 (54%) | Loss: 0.357444\n",
      "Train Epoch: 7 | Batch Status: 33280/60000 (55%) | Loss: 0.074662\n",
      "Train Epoch: 7 | Batch Status: 33920/60000 (57%) | Loss: 0.086369\n",
      "Train Epoch: 7 | Batch Status: 34560/60000 (58%) | Loss: 0.248888\n",
      "Train Epoch: 7 | Batch Status: 35200/60000 (59%) | Loss: 0.080392\n",
      "Train Epoch: 7 | Batch Status: 35840/60000 (60%) | Loss: 0.159191\n",
      "Train Epoch: 7 | Batch Status: 36480/60000 (61%) | Loss: 0.066114\n",
      "Train Epoch: 7 | Batch Status: 37120/60000 (62%) | Loss: 0.120534\n",
      "Train Epoch: 7 | Batch Status: 37760/60000 (63%) | Loss: 0.194856\n",
      "Train Epoch: 7 | Batch Status: 38400/60000 (64%) | Loss: 0.048069\n",
      "Train Epoch: 7 | Batch Status: 39040/60000 (65%) | Loss: 0.069854\n",
      "Train Epoch: 7 | Batch Status: 39680/60000 (66%) | Loss: 0.078715\n",
      "Train Epoch: 7 | Batch Status: 40320/60000 (67%) | Loss: 0.165581\n",
      "Train Epoch: 7 | Batch Status: 40960/60000 (68%) | Loss: 0.134628\n",
      "Train Epoch: 7 | Batch Status: 41600/60000 (69%) | Loss: 0.039262\n",
      "Train Epoch: 7 | Batch Status: 42240/60000 (70%) | Loss: 0.110447\n",
      "Train Epoch: 7 | Batch Status: 42880/60000 (71%) | Loss: 0.064857\n",
      "Train Epoch: 7 | Batch Status: 43520/60000 (72%) | Loss: 0.153807\n",
      "Train Epoch: 7 | Batch Status: 44160/60000 (74%) | Loss: 0.070738\n",
      "Train Epoch: 7 | Batch Status: 44800/60000 (75%) | Loss: 0.165181\n",
      "Train Epoch: 7 | Batch Status: 45440/60000 (76%) | Loss: 0.078741\n",
      "Train Epoch: 7 | Batch Status: 46080/60000 (77%) | Loss: 0.082587\n",
      "Train Epoch: 7 | Batch Status: 46720/60000 (78%) | Loss: 0.121477\n",
      "Train Epoch: 7 | Batch Status: 47360/60000 (79%) | Loss: 0.100382\n",
      "Train Epoch: 7 | Batch Status: 48000/60000 (80%) | Loss: 0.045734\n",
      "Train Epoch: 7 | Batch Status: 48640/60000 (81%) | Loss: 0.088701\n",
      "Train Epoch: 7 | Batch Status: 49280/60000 (82%) | Loss: 0.032114\n",
      "Train Epoch: 7 | Batch Status: 49920/60000 (83%) | Loss: 0.144999\n",
      "Train Epoch: 7 | Batch Status: 50560/60000 (84%) | Loss: 0.060088\n",
      "Train Epoch: 7 | Batch Status: 51200/60000 (85%) | Loss: 0.173078\n",
      "Train Epoch: 7 | Batch Status: 51840/60000 (86%) | Loss: 0.086866\n",
      "Train Epoch: 7 | Batch Status: 52480/60000 (87%) | Loss: 0.031309\n",
      "Train Epoch: 7 | Batch Status: 53120/60000 (88%) | Loss: 0.047516\n",
      "Train Epoch: 7 | Batch Status: 53760/60000 (90%) | Loss: 0.068154\n",
      "Train Epoch: 7 | Batch Status: 54400/60000 (91%) | Loss: 0.032280\n",
      "Train Epoch: 7 | Batch Status: 55040/60000 (92%) | Loss: 0.060035\n",
      "Train Epoch: 7 | Batch Status: 55680/60000 (93%) | Loss: 0.022749\n",
      "Train Epoch: 7 | Batch Status: 56320/60000 (94%) | Loss: 0.078586\n",
      "Train Epoch: 7 | Batch Status: 56960/60000 (95%) | Loss: 0.025930\n",
      "Train Epoch: 7 | Batch Status: 57600/60000 (96%) | Loss: 0.083022\n",
      "Train Epoch: 7 | Batch Status: 58240/60000 (97%) | Loss: 0.083055\n",
      "Train Epoch: 7 | Batch Status: 58880/60000 (98%) | Loss: 0.075021\n",
      "Train Epoch: 7 | Batch Status: 59520/60000 (99%) | Loss: 0.085000\n",
      "Training time: 0m 10s\n",
      "===========================\n",
      "Test set: Average loss: 0.0019, Accuracy: 9650/10000 (96%)\n",
      "Testing time: 0m 11s\n",
      "Train Epoch: 8 | Batch Status: 0/60000 (0%) | Loss: 0.193398\n",
      "Train Epoch: 8 | Batch Status: 640/60000 (1%) | Loss: 0.056324\n",
      "Train Epoch: 8 | Batch Status: 1280/60000 (2%) | Loss: 0.049287\n",
      "Train Epoch: 8 | Batch Status: 1920/60000 (3%) | Loss: 0.080817\n",
      "Train Epoch: 8 | Batch Status: 2560/60000 (4%) | Loss: 0.097973\n",
      "Train Epoch: 8 | Batch Status: 3200/60000 (5%) | Loss: 0.171510\n",
      "Train Epoch: 8 | Batch Status: 3840/60000 (6%) | Loss: 0.085009\n",
      "Train Epoch: 8 | Batch Status: 4480/60000 (7%) | Loss: 0.199009\n",
      "Train Epoch: 8 | Batch Status: 5120/60000 (9%) | Loss: 0.138289\n",
      "Train Epoch: 8 | Batch Status: 5760/60000 (10%) | Loss: 0.046591\n",
      "Train Epoch: 8 | Batch Status: 6400/60000 (11%) | Loss: 0.080013\n",
      "Train Epoch: 8 | Batch Status: 7040/60000 (12%) | Loss: 0.065820\n",
      "Train Epoch: 8 | Batch Status: 7680/60000 (13%) | Loss: 0.049704\n",
      "Train Epoch: 8 | Batch Status: 8320/60000 (14%) | Loss: 0.061379\n",
      "Train Epoch: 8 | Batch Status: 8960/60000 (15%) | Loss: 0.112059\n",
      "Train Epoch: 8 | Batch Status: 9600/60000 (16%) | Loss: 0.059657\n",
      "Train Epoch: 8 | Batch Status: 10240/60000 (17%) | Loss: 0.052811\n",
      "Train Epoch: 8 | Batch Status: 10880/60000 (18%) | Loss: 0.093973\n",
      "Train Epoch: 8 | Batch Status: 11520/60000 (19%) | Loss: 0.058120\n",
      "Train Epoch: 8 | Batch Status: 12160/60000 (20%) | Loss: 0.160577\n",
      "Train Epoch: 8 | Batch Status: 12800/60000 (21%) | Loss: 0.217411\n",
      "Train Epoch: 8 | Batch Status: 13440/60000 (22%) | Loss: 0.163805\n",
      "Train Epoch: 8 | Batch Status: 14080/60000 (23%) | Loss: 0.071142\n",
      "Train Epoch: 8 | Batch Status: 14720/60000 (25%) | Loss: 0.140509\n",
      "Train Epoch: 8 | Batch Status: 15360/60000 (26%) | Loss: 0.134817\n",
      "Train Epoch: 8 | Batch Status: 16000/60000 (27%) | Loss: 0.119177\n",
      "Train Epoch: 8 | Batch Status: 16640/60000 (28%) | Loss: 0.099779\n",
      "Train Epoch: 8 | Batch Status: 17280/60000 (29%) | Loss: 0.186924\n",
      "Train Epoch: 8 | Batch Status: 17920/60000 (30%) | Loss: 0.015431\n",
      "Train Epoch: 8 | Batch Status: 18560/60000 (31%) | Loss: 0.090559\n",
      "Train Epoch: 8 | Batch Status: 19200/60000 (32%) | Loss: 0.130072\n",
      "Train Epoch: 8 | Batch Status: 19840/60000 (33%) | Loss: 0.115658\n",
      "Train Epoch: 8 | Batch Status: 20480/60000 (34%) | Loss: 0.129907\n",
      "Train Epoch: 8 | Batch Status: 21120/60000 (35%) | Loss: 0.106419\n",
      "Train Epoch: 8 | Batch Status: 21760/60000 (36%) | Loss: 0.190265\n",
      "Train Epoch: 8 | Batch Status: 22400/60000 (37%) | Loss: 0.093186\n",
      "Train Epoch: 8 | Batch Status: 23040/60000 (38%) | Loss: 0.079990\n",
      "Train Epoch: 8 | Batch Status: 23680/60000 (39%) | Loss: 0.165968\n",
      "Train Epoch: 8 | Batch Status: 24320/60000 (41%) | Loss: 0.056519\n",
      "Train Epoch: 8 | Batch Status: 24960/60000 (42%) | Loss: 0.067041\n",
      "Train Epoch: 8 | Batch Status: 25600/60000 (43%) | Loss: 0.071826\n",
      "Train Epoch: 8 | Batch Status: 26240/60000 (44%) | Loss: 0.034855\n",
      "Train Epoch: 8 | Batch Status: 26880/60000 (45%) | Loss: 0.041459\n",
      "Train Epoch: 8 | Batch Status: 27520/60000 (46%) | Loss: 0.134890\n",
      "Train Epoch: 8 | Batch Status: 28160/60000 (47%) | Loss: 0.127974\n",
      "Train Epoch: 8 | Batch Status: 28800/60000 (48%) | Loss: 0.062349\n",
      "Train Epoch: 8 | Batch Status: 29440/60000 (49%) | Loss: 0.073267\n",
      "Train Epoch: 8 | Batch Status: 30080/60000 (50%) | Loss: 0.243435\n",
      "Train Epoch: 8 | Batch Status: 30720/60000 (51%) | Loss: 0.138577\n",
      "Train Epoch: 8 | Batch Status: 31360/60000 (52%) | Loss: 0.028552\n",
      "Train Epoch: 8 | Batch Status: 32000/60000 (53%) | Loss: 0.200163\n",
      "Train Epoch: 8 | Batch Status: 32640/60000 (54%) | Loss: 0.136083\n",
      "Train Epoch: 8 | Batch Status: 33280/60000 (55%) | Loss: 0.078549\n",
      "Train Epoch: 8 | Batch Status: 33920/60000 (57%) | Loss: 0.044772\n",
      "Train Epoch: 8 | Batch Status: 34560/60000 (58%) | Loss: 0.051604\n",
      "Train Epoch: 8 | Batch Status: 35200/60000 (59%) | Loss: 0.101065\n",
      "Train Epoch: 8 | Batch Status: 35840/60000 (60%) | Loss: 0.034446\n",
      "Train Epoch: 8 | Batch Status: 36480/60000 (61%) | Loss: 0.075228\n",
      "Train Epoch: 8 | Batch Status: 37120/60000 (62%) | Loss: 0.144178\n",
      "Train Epoch: 8 | Batch Status: 37760/60000 (63%) | Loss: 0.032547\n",
      "Train Epoch: 8 | Batch Status: 38400/60000 (64%) | Loss: 0.103630\n",
      "Train Epoch: 8 | Batch Status: 39040/60000 (65%) | Loss: 0.053375\n",
      "Train Epoch: 8 | Batch Status: 39680/60000 (66%) | Loss: 0.105228\n",
      "Train Epoch: 8 | Batch Status: 40320/60000 (67%) | Loss: 0.059984\n",
      "Train Epoch: 8 | Batch Status: 40960/60000 (68%) | Loss: 0.087121\n",
      "Train Epoch: 8 | Batch Status: 41600/60000 (69%) | Loss: 0.170703\n",
      "Train Epoch: 8 | Batch Status: 42240/60000 (70%) | Loss: 0.105735\n",
      "Train Epoch: 8 | Batch Status: 42880/60000 (71%) | Loss: 0.090162\n",
      "Train Epoch: 8 | Batch Status: 43520/60000 (72%) | Loss: 0.114575\n",
      "Train Epoch: 8 | Batch Status: 44160/60000 (74%) | Loss: 0.054330\n",
      "Train Epoch: 8 | Batch Status: 44800/60000 (75%) | Loss: 0.109196\n",
      "Train Epoch: 8 | Batch Status: 45440/60000 (76%) | Loss: 0.104225\n",
      "Train Epoch: 8 | Batch Status: 46080/60000 (77%) | Loss: 0.037119\n",
      "Train Epoch: 8 | Batch Status: 46720/60000 (78%) | Loss: 0.071141\n",
      "Train Epoch: 8 | Batch Status: 47360/60000 (79%) | Loss: 0.123639\n",
      "Train Epoch: 8 | Batch Status: 48000/60000 (80%) | Loss: 0.264015\n",
      "Train Epoch: 8 | Batch Status: 48640/60000 (81%) | Loss: 0.025221\n",
      "Train Epoch: 8 | Batch Status: 49280/60000 (82%) | Loss: 0.306523\n",
      "Train Epoch: 8 | Batch Status: 49920/60000 (83%) | Loss: 0.073547\n",
      "Train Epoch: 8 | Batch Status: 50560/60000 (84%) | Loss: 0.091588\n",
      "Train Epoch: 8 | Batch Status: 51200/60000 (85%) | Loss: 0.098086\n",
      "Train Epoch: 8 | Batch Status: 51840/60000 (86%) | Loss: 0.051306\n",
      "Train Epoch: 8 | Batch Status: 52480/60000 (87%) | Loss: 0.038797\n",
      "Train Epoch: 8 | Batch Status: 53120/60000 (88%) | Loss: 0.058133\n",
      "Train Epoch: 8 | Batch Status: 53760/60000 (90%) | Loss: 0.062418\n",
      "Train Epoch: 8 | Batch Status: 54400/60000 (91%) | Loss: 0.073095\n",
      "Train Epoch: 8 | Batch Status: 55040/60000 (92%) | Loss: 0.239466\n",
      "Train Epoch: 8 | Batch Status: 55680/60000 (93%) | Loss: 0.099118\n",
      "Train Epoch: 8 | Batch Status: 56320/60000 (94%) | Loss: 0.104996\n",
      "Train Epoch: 8 | Batch Status: 56960/60000 (95%) | Loss: 0.145693\n",
      "Train Epoch: 8 | Batch Status: 57600/60000 (96%) | Loss: 0.042837\n",
      "Train Epoch: 8 | Batch Status: 58240/60000 (97%) | Loss: 0.044021\n",
      "Train Epoch: 8 | Batch Status: 58880/60000 (98%) | Loss: 0.068899\n",
      "Train Epoch: 8 | Batch Status: 59520/60000 (99%) | Loss: 0.133540\n",
      "Training time: 0m 10s\n",
      "===========================\n",
      "Test set: Average loss: 0.0018, Accuracy: 9661/10000 (97%)\n",
      "Testing time: 0m 11s\n",
      "Train Epoch: 9 | Batch Status: 0/60000 (0%) | Loss: 0.035523\n",
      "Train Epoch: 9 | Batch Status: 640/60000 (1%) | Loss: 0.139139\n",
      "Train Epoch: 9 | Batch Status: 1280/60000 (2%) | Loss: 0.032799\n",
      "Train Epoch: 9 | Batch Status: 1920/60000 (3%) | Loss: 0.041825\n",
      "Train Epoch: 9 | Batch Status: 2560/60000 (4%) | Loss: 0.119786\n",
      "Train Epoch: 9 | Batch Status: 3200/60000 (5%) | Loss: 0.173839\n",
      "Train Epoch: 9 | Batch Status: 3840/60000 (6%) | Loss: 0.134822\n",
      "Train Epoch: 9 | Batch Status: 4480/60000 (7%) | Loss: 0.059273\n",
      "Train Epoch: 9 | Batch Status: 5120/60000 (9%) | Loss: 0.144864\n",
      "Train Epoch: 9 | Batch Status: 5760/60000 (10%) | Loss: 0.075247\n",
      "Train Epoch: 9 | Batch Status: 6400/60000 (11%) | Loss: 0.156393\n",
      "Train Epoch: 9 | Batch Status: 7040/60000 (12%) | Loss: 0.086353\n",
      "Train Epoch: 9 | Batch Status: 7680/60000 (13%) | Loss: 0.093228\n",
      "Train Epoch: 9 | Batch Status: 8320/60000 (14%) | Loss: 0.068315\n",
      "Train Epoch: 9 | Batch Status: 8960/60000 (15%) | Loss: 0.038140\n",
      "Train Epoch: 9 | Batch Status: 9600/60000 (16%) | Loss: 0.069030\n",
      "Train Epoch: 9 | Batch Status: 10240/60000 (17%) | Loss: 0.077762\n",
      "Train Epoch: 9 | Batch Status: 10880/60000 (18%) | Loss: 0.039407\n",
      "Train Epoch: 9 | Batch Status: 11520/60000 (19%) | Loss: 0.098184\n",
      "Train Epoch: 9 | Batch Status: 12160/60000 (20%) | Loss: 0.217977\n",
      "Train Epoch: 9 | Batch Status: 12800/60000 (21%) | Loss: 0.166092\n",
      "Train Epoch: 9 | Batch Status: 13440/60000 (22%) | Loss: 0.043586\n",
      "Train Epoch: 9 | Batch Status: 14080/60000 (23%) | Loss: 0.093836\n",
      "Train Epoch: 9 | Batch Status: 14720/60000 (25%) | Loss: 0.042046\n",
      "Train Epoch: 9 | Batch Status: 15360/60000 (26%) | Loss: 0.077202\n",
      "Train Epoch: 9 | Batch Status: 16000/60000 (27%) | Loss: 0.023459\n",
      "Train Epoch: 9 | Batch Status: 16640/60000 (28%) | Loss: 0.067140\n",
      "Train Epoch: 9 | Batch Status: 17280/60000 (29%) | Loss: 0.075119\n",
      "Train Epoch: 9 | Batch Status: 17920/60000 (30%) | Loss: 0.026993\n",
      "Train Epoch: 9 | Batch Status: 18560/60000 (31%) | Loss: 0.075721\n",
      "Train Epoch: 9 | Batch Status: 19200/60000 (32%) | Loss: 0.132645\n",
      "Train Epoch: 9 | Batch Status: 19840/60000 (33%) | Loss: 0.160582\n",
      "Train Epoch: 9 | Batch Status: 20480/60000 (34%) | Loss: 0.065836\n",
      "Train Epoch: 9 | Batch Status: 21120/60000 (35%) | Loss: 0.045389\n",
      "Train Epoch: 9 | Batch Status: 21760/60000 (36%) | Loss: 0.193108\n",
      "Train Epoch: 9 | Batch Status: 22400/60000 (37%) | Loss: 0.046987\n",
      "Train Epoch: 9 | Batch Status: 23040/60000 (38%) | Loss: 0.093617\n",
      "Train Epoch: 9 | Batch Status: 23680/60000 (39%) | Loss: 0.079097\n",
      "Train Epoch: 9 | Batch Status: 24320/60000 (41%) | Loss: 0.014057\n",
      "Train Epoch: 9 | Batch Status: 24960/60000 (42%) | Loss: 0.085532\n",
      "Train Epoch: 9 | Batch Status: 25600/60000 (43%) | Loss: 0.069864\n",
      "Train Epoch: 9 | Batch Status: 26240/60000 (44%) | Loss: 0.065813\n",
      "Train Epoch: 9 | Batch Status: 26880/60000 (45%) | Loss: 0.190140\n",
      "Train Epoch: 9 | Batch Status: 27520/60000 (46%) | Loss: 0.063283\n",
      "Train Epoch: 9 | Batch Status: 28160/60000 (47%) | Loss: 0.013557\n",
      "Train Epoch: 9 | Batch Status: 28800/60000 (48%) | Loss: 0.051329\n",
      "Train Epoch: 9 | Batch Status: 29440/60000 (49%) | Loss: 0.041233\n",
      "Train Epoch: 9 | Batch Status: 30080/60000 (50%) | Loss: 0.038471\n",
      "Train Epoch: 9 | Batch Status: 30720/60000 (51%) | Loss: 0.045615\n",
      "Train Epoch: 9 | Batch Status: 31360/60000 (52%) | Loss: 0.099921\n",
      "Train Epoch: 9 | Batch Status: 32000/60000 (53%) | Loss: 0.061858\n",
      "Train Epoch: 9 | Batch Status: 32640/60000 (54%) | Loss: 0.033913\n",
      "Train Epoch: 9 | Batch Status: 33280/60000 (55%) | Loss: 0.098684\n",
      "Train Epoch: 9 | Batch Status: 33920/60000 (57%) | Loss: 0.036082\n",
      "Train Epoch: 9 | Batch Status: 34560/60000 (58%) | Loss: 0.032821\n",
      "Train Epoch: 9 | Batch Status: 35200/60000 (59%) | Loss: 0.176564\n",
      "Train Epoch: 9 | Batch Status: 35840/60000 (60%) | Loss: 0.187786\n",
      "Train Epoch: 9 | Batch Status: 36480/60000 (61%) | Loss: 0.033717\n",
      "Train Epoch: 9 | Batch Status: 37120/60000 (62%) | Loss: 0.089957\n",
      "Train Epoch: 9 | Batch Status: 37760/60000 (63%) | Loss: 0.086960\n",
      "Train Epoch: 9 | Batch Status: 38400/60000 (64%) | Loss: 0.067132\n",
      "Train Epoch: 9 | Batch Status: 39040/60000 (65%) | Loss: 0.174823\n",
      "Train Epoch: 9 | Batch Status: 39680/60000 (66%) | Loss: 0.013242\n",
      "Train Epoch: 9 | Batch Status: 40320/60000 (67%) | Loss: 0.088629\n",
      "Train Epoch: 9 | Batch Status: 40960/60000 (68%) | Loss: 0.071526\n",
      "Train Epoch: 9 | Batch Status: 41600/60000 (69%) | Loss: 0.097988\n",
      "Train Epoch: 9 | Batch Status: 42240/60000 (70%) | Loss: 0.117737\n",
      "Train Epoch: 9 | Batch Status: 42880/60000 (71%) | Loss: 0.016867\n",
      "Train Epoch: 9 | Batch Status: 43520/60000 (72%) | Loss: 0.161557\n",
      "Train Epoch: 9 | Batch Status: 44160/60000 (74%) | Loss: 0.037580\n",
      "Train Epoch: 9 | Batch Status: 44800/60000 (75%) | Loss: 0.198643\n",
      "Train Epoch: 9 | Batch Status: 45440/60000 (76%) | Loss: 0.211138\n",
      "Train Epoch: 9 | Batch Status: 46080/60000 (77%) | Loss: 0.010315\n",
      "Train Epoch: 9 | Batch Status: 46720/60000 (78%) | Loss: 0.100550\n",
      "Train Epoch: 9 | Batch Status: 47360/60000 (79%) | Loss: 0.044153\n",
      "Train Epoch: 9 | Batch Status: 48000/60000 (80%) | Loss: 0.170423\n",
      "Train Epoch: 9 | Batch Status: 48640/60000 (81%) | Loss: 0.055160\n",
      "Train Epoch: 9 | Batch Status: 49280/60000 (82%) | Loss: 0.137626\n",
      "Train Epoch: 9 | Batch Status: 49920/60000 (83%) | Loss: 0.034949\n",
      "Train Epoch: 9 | Batch Status: 50560/60000 (84%) | Loss: 0.030701\n",
      "Train Epoch: 9 | Batch Status: 51200/60000 (85%) | Loss: 0.057721\n",
      "Train Epoch: 9 | Batch Status: 51840/60000 (86%) | Loss: 0.088019\n",
      "Train Epoch: 9 | Batch Status: 52480/60000 (87%) | Loss: 0.117674\n",
      "Train Epoch: 9 | Batch Status: 53120/60000 (88%) | Loss: 0.065359\n",
      "Train Epoch: 9 | Batch Status: 53760/60000 (90%) | Loss: 0.155786\n",
      "Train Epoch: 9 | Batch Status: 54400/60000 (91%) | Loss: 0.061912\n",
      "Train Epoch: 9 | Batch Status: 55040/60000 (92%) | Loss: 0.181498\n",
      "Train Epoch: 9 | Batch Status: 55680/60000 (93%) | Loss: 0.077854\n",
      "Train Epoch: 9 | Batch Status: 56320/60000 (94%) | Loss: 0.089923\n",
      "Train Epoch: 9 | Batch Status: 56960/60000 (95%) | Loss: 0.048328\n",
      "Train Epoch: 9 | Batch Status: 57600/60000 (96%) | Loss: 0.079017\n",
      "Train Epoch: 9 | Batch Status: 58240/60000 (97%) | Loss: 0.244397\n",
      "Train Epoch: 9 | Batch Status: 58880/60000 (98%) | Loss: 0.096124\n",
      "Train Epoch: 9 | Batch Status: 59520/60000 (99%) | Loss: 0.057448\n",
      "Training time: 0m 10s\n",
      "===========================\n",
      "Test set: Average loss: 0.0016, Accuracy: 9695/10000 (97%)\n",
      "Testing time: 0m 11s\n",
      "Total Time: 1m 41s\n",
      "Model was trained on cpu!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    since = time.time()\n",
    "    for epoch in range(1, 10):\n",
    "        epoch_start = time.time()\n",
    "        train(epoch)\n",
    "        m, s = divmod(time.time() - epoch_start, 60)\n",
    "        print(f'Training time: {m:.0f}m {s:.0f}s')\n",
    "        test()\n",
    "        m, s = divmod(time.time() - epoch_start, 60)\n",
    "        print(f'Testing time: {m:.0f}m {s:.0f}s')\n",
    "\n",
    "    m, s = divmod(time.time() - since, 60)\n",
    "    print(f'Total Time: {m:.0f}m {s:.0f}s\\nModel was trained on {device}!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}